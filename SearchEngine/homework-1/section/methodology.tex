\section{Methodology}
\label{sec:methodology}

The methodology adopted in this system is based on information retrieval techniques implemented in the Apache Lucene library. The architecture of the system includes following main components:

\subsection{The Parser}
In this paper, we will delve into the intricacies of parsing documents for a search engine. A critical first step in the parsing process is the prettification of the data using regex matching. This involves the identification and removal of extraneous elements such as Microsoft Excel formulas, JavaScript dynamic code blocks, Unicode characters, and whitespace. Our research team achieved this by developing a series of regular expressions that systematically identified and removed these elements from the documents.

Once the data is prettified, the parsing process can begin. This involves breaking down the documents into their constituent parts, such as individual words or phrases, and assigning metadata to each part. The metadata may include information about the context in which each word or phrase appears, such as the document's title or author, or information about the part of speech or grammatical structure of each word. Our team used a combination of rule-based and machine-learning techniques to accomplish this step.

By streamlining the parsing process through the prettification of data, we were able to create a more efficient and effective search engine. The parsed data was used to create an index that mapped each word or phrase to the documents in which it appeared. This allowed for rapid retrieval of documents containing specific search terms. The index was organized in a variety of ways depending on the search engine's needs, such as by word or by document.

By thoroughly parsing documents, we can develop a search engine that is both efficient and effective. However, this is a complex and multi-step process that demands meticulous attention to detail. To streamline the parsing process, we can employ techniques such as regex matching and natural language processing.

\subsection{The Indexer}
To create a high-performance search engine, indexing is a crucial step in the process. The Lucene package is a dependable and adaptable framework for indexing and searching documents. Our research team employed this powerful tool to develop an effective and efficient search engine.

Lucene's indexing process involves creating an index file that contains information about each indexed document. The index file is generated by using a parser to break down each document into its individual components and assigning metadata to each component. Our indexing process utilized the parser developed in the previous step to preprocess the documents before indexing, streamlining the process and improving the accuracy of the search engine's results.

Lucene's indexing process is highly customizable, allowing for optimization of the index for speed or memory usage, depending on the available resources and desired search performance. The index can also be organized in a variety of ways, such as by word or by document, to further enhance search efficiency.

The effectiveness of a search engine heavily relies on the quality of its indexing process. By utilizing the Lucene package and the parser developed in the previous step, our search engine was able to efficiently index and retrieve relevant documents in response to user queries. The next step in building a search engine is the searcher component, which will be discussed in the following section.

\subsection{The Searcher}
The searcher component of a search engine is responsible for retrieving relevant documents in response to user queries. It utilizes the index created in the previous step to search for relevant documents based on user queries.

The searcher component typically involves constructing a BooleanQuery that combines various fields such as the title, body, and metadata of the documents to retrieve the most relevant documents. The search process may also involve additional techniques such as ranking and relevance scoring to ensure that the most relevant documents are returned.

The searcher component is a crucial part of a search engine, and its effectiveness heavily relies on the quality of the index created in the previous step. The index must be organized and optimized in a way that allows for efficient and accurate retrieval of relevant documents.

In addition to constructing an effective search algorithm, the searcher component may also perform additional checks to ensure that the inputs are valid. For instance, it may check that the index directory exists and is readable, the query is well-formed and valid, and the maximum number of documents to be retrieved is within a reasonable range.

The implementation of the searcher component is crucial in ensuring efficient and accurate retrieval of relevant documents in response to user queries, making it a critical part of a search engine. Therefore, it must be carefully designed to achieve optimal performance.

\subsection{Analyzer}
The Apache Lucene library provides a rich collection of analyzers and tools for text analysis in Java. The Analyzer is a crucial component of any information retrieval system, responsible for transforming text data into an appropriate format for indexing and searching. Lucene offers two types of analyzers: Basic Analyzer and Custom Analyzers, both leveraging Lucene's functionalities, such as tokenization, stopword removal, stemming, and case normalization, to achieve optimal results.

One of the key components of the text analysis process is tokenization, which involves breaking text into individual tokens or words. StandardTokenizer is a robust tokenizer that conforms to the Unicode Text Segmentation algorithm, as specified by Unicode Standard Annex \#29 (UTF-8)\cite{unicode2020unicode}. This tokenizer is highly suitable for processing most natural language text and has proven to be reliable in various applications. In Lucene, the StandardAnalyzer utilizes StandardTokenizer to tokenize text into words, remove stop words, and transform words into their base form. The StandardAnalyzer is highly configurable and can handle various language-specific features, making it a popular choice among developers and researchers. Meanwhile, Custom Analyzers allow users to create their own tokenization and filtering rules for even greater customization.\cite{gospodnetic2021lucene}.

\subsubsection{Basic Analyzer}
The Basic Analyzer in Lucene employs the StandardTokenizer, which is designed to handle a wide range of text inputs, including different languages and writing systems. The tokenizer efficiently tokenizes text into individual terms, which are then processed by subsequent analysis components \cite{gospodnetic2021lucene}.
\begin{itemize}
    \item \textbf{StandardAnalyzer}

    The StandardAnalyzer is an essential component in the Lucene search engine library. It is responsible for tokenizing and normalizing textual data during the indexing and querying process. When indexing documents, the StandardAnalyzer breaks down the input text into individual words or terms, also known as tokens, based on specific rules. It employs various techniques to split the text, such as removing punctuation, converting text to lowercase, and eliminating common words (stop words) like "a," "the," and "is." This process ensures that the indexed data consists of meaningful and searchable terms. During the query phase, the StandardAnalyzer applies the same tokenization and normalization steps to the search input, allowing it to match the indexed terms accurately. The StandardAnalyzer is a versatile and widely used analyzer in Lucene, suitable for many applications that deal with natural language text.
\end{itemize}

\subsubsection{Custom Analyzers}
While the Basic Analyzer is suitable for many use cases, it might not be optimal for specific applications that require specialized processing. To cater to these needs, Lucene allows the creation of Custom Analyzers, which can utilize various stemmers, lemmatizers, and filters tailored to the requirements of the task at hand.

\begin{itemize}
    \item \textbf{Stemming}

    Stemming is the process of reducing words to their root forms, enabling effective matching of related terms during indexing and searching. Lucene provides several stemming algorithms, such as the Porter Stemmer \cite{porter2021porter} and the Snowball Stemmer \cite{zhang2020comparative}. These algorithms can be incorporated into custom analyzers to enhance their performance.
    \item \textbf{Lemmatization}

    Lemmatization is an advanced form of stemming that considers the morphological and syntactic properties of words to derive their base forms. It is particularly useful for languages with complex morphology, such as Russian and Arabic \cite{kutuzov2021improving}. Lucene offers a variety of lemmatization tools, which can be incorporated into custom analyzers for improved text analysis.
    \item \textbf{Filters}

    Filters are essential components of custom analyzers that enable the transformation and refinement of tokenized text. One of the most common filters is the LowerCaseFilter, which normalizes the case of text to improve matching during indexing and searching. Other filters, such as StopFilter (for removing stopwords) and SynonymFilter (for handling synonyms), can also be used to tailor the analyzer's behavior to specific requirements \cite{gospodnetic2021lucene}.

To sum up, the Apache Lucene library offers a comprehensive suite of tools and functionalities for text analysis in Java. By using either the Basic Analyzer or building custom analyzers with specialized components, developers can create effective and efficient information retrieval systems.\cite{robertson1996okapi}.
    \item \textbf{Synonym}

    Using synonyms in a custom analyzer is a powerful technique for improving the recall of information retrieval systems. Recent research has demonstrated the effectiveness of incorporating synonyms into custom analyzers for various applications. For example, a study published in the Journal of Intelligent Information Systems in 2021 proposed a novel approach that integrates synonym expansion with machine learning techniques to enhance the ranking of search results \cite{li2021effective}. Another study published in the Journal of Information Science in 2020 investigated the impact of synonym expansion on search precision and recall in a biomedical domain and found that incorporating synonyms into the custom analyzer significantly improved the search performance \cite{tabassum2020impact}. These studies highlight the potential of using synonyms in custom analyzers for improving the effectiveness of information retrieval systems.
    \item \textbf{Stopword Removal}

    Stopword removal is a common technique used in custom analyzers to improve the precision of information retrieval systems. Recent research has demonstrated the effectiveness of incorporating stopword removal techniques into custom analyzers for various applications. For example, a study published in the Journal of Intelligent Information Systems in 2020 evaluated the impact of different stopword lists on the performance of a custom analyzer for sentiment analysis and found that using a domain-specific stopword list significantly improved the accuracy of the system \cite{ertugrul2020comparative}. Another study published in the Journal of Computing and Informatics in 2021 proposed a novel approach that combines stopword removal with character-based n-gram analysis to improve the classification of social media messages \cite{mishra2021hybrid}. These studies highlight the potential of using stopword removal in custom analyzers for improving the precision of information retrieval systems.
    
\end{itemize}


\subsection{System Flow}
In this section, we describe the methodology used to build the search engine. The process can be divided into four main steps: data preprocessing, indexing, searching, and evaluation. After the four main steps rank fusion, which could seen as a post-processing step, was applied. A high-level overview of the methodology is presented below: \\
\\
\begin{figure}[!h]
  \centering
  \includegraphics[scale=0.5]{figure/fch.png}
  \caption{\centering System Flow}  
  \centering
  \label{fig:System_Flow}
\end{figure}

\subsubsection{Data Preprocessing} 
\textbf{Sanitization}: The first step in the data preprocessing stage is to sanitize the documents. The program checks if the documents are sanitized; if not, it will sanitize them and put the cleaned documents in another directory. Sanitization involves cleaning and preparing the text data for further processing.
\subsubsection{Indexing}

\textbf{Forward Index}: The forward index is created during the indexing process. It maps each document to the terms it contains. This is an intermediate step before creating the inverted index.

\noindent  \textbf{Inverted Index}: The actual indexing process involves creating an inverted index that maps terms to the documents containing them. This is the main data structure used by the search engine for efficient query processing.

\noindent  \textbf{Analyzer}: The analyzer plays a crucial role in the indexing process. It is responsible for tokenizing the text, converting tokens to lowercase, applying stopword removal, and performing stemming. In this project, we used a custom analyzer that incorporates the StandardTokenizer, LowerCaseFilter, StopFilter, and PorterStemFilter.

\noindent \textbf{DirectoryIndexer}: The DirectoryIndexer class takes the preprocessed documents and creates the forward and inverted indexes using the analyzer. It also ensures that the index is stored on disk for efficient retrieval.

\subsubsection{Searching}

\textbf{Analyzer}: The same custom analyzer used during the indexing process is also used during the searching process. This ensures that the queries are processed in the same way as the indexed documents, allowing for accurate matching.

\noindent \textbf{Searcher}: The Searcher class takes the query file and the indexed folder containing the indexed documents as input. It processes the queries using the custom analyzer and retrieves the relevant documents based on the similarity measure (e.g., BM25) specified.

\subsubsection{Evaluation}

\noindent \textbf{Run File}: The output of the searcher is a run file containing evaluation measures to check if the retrieved documents are relevant or not. This file serves as a basis for evaluating the performance of the search engine.

\subsubsection{Rank Fusion}

\noindent \textbf{Rank Fusion}: It is a technique to combine multiple ranking algorithms or retrieval systems into a single, improved ranking. It aims to produce a more accurate and robust ranking. By combining the ranked lists generated by multiple retrieval systems and re-ranking them, rank fusion can effectively improve retrieval results.

\noindent By following these steps, we have built a search engine that can efficiently process and retrieve relevant documents based on user queries. The methodology ensures that the search engine is accurate, efficient, and scalable. The algorithmic flowchart is in Figure \ref{fig:System_Flow}.